#!/usr/bin/env python

from __future__ import print_function

import json
import os
import pickle
import sys
import traceback

import pandas as pd
from numpy import mean
from sklearn import tree
from sklearn.model_selection import cross_val_score

import os
import sys
import json
import pickle
import datetime
import argparse
import traceback
import numpy as np
import pandas as pd
from pathlib import Path

from artwork_content_recsys import ArtworkContent

import nltk

# These are the paths to where SageMaker mounts interesting things in your container.

prefix = '/opt/ml/'

input_path = prefix + 'input/data'
output_path = os.path.join(prefix, 'output')
model_path = os.path.join(prefix, 'model')
param_path = os.path.join(prefix, 'input/config/hyperparameters.json')

# This algorithm has a single channel of input data called 'training'. Since we run in
# File mode, the input files are copied to the directory specified here.
channel_name='training'
training_path = os.path.join(input_path, channel_name)


def initialize_nltk():    
    nltk.download('averaged_perceptron_tagger')
    nltk.download('wordnet')
    nltk.download('omw-1.4')
    nltk.download('stopwords')


def train_artwork_content(dataFrame):
    """
    
    Artwork content model training
    
    """
    artwork_model = ArtworkContent(dataFrame)
    artwork_model.fit()
    return artwork_model.result


def main(content_df):

    initialize_nltk()
    
    artwork_content_model = train_artwork_content(content_df)
    return artwork_content_model


# The function to execute the training.
def train():
    print('Starting the training.')
    try:
        # Read in any hyperparameters that the user passed with the training job
        with open(param_path, 'r') as tc:
            trainingParams = json.load(tc)
            
        print(training_path)
        

        # Take the set of files and read them all into a single pandas dataframe
        input_files = [ os.path.join(training_path, file) for file in os.listdir(training_path) ][0]
                                      
        
        print(input_files)
        raw_data = pd.read_csv(input_files)
        #raw_data = [ pd.read_csv(file, header=None) for file in input_files ]
        print(raw_data)
        
        #train_data = pd.concat(raw_data)

        artwork_content_model = main(raw_data)
        
        print('TYPE', artwork_content_model)
        print('data', artwork_content_model)

        # save the model
        #with open(os.path.join(model_path, 'artworkcontentmodel.pkl'), 'w') as out:
        #    pickle.dump(model, out)
        #print('Training complete.')
        
        
        artwork_filename = os.path.join(model_path, 'artwork_content_model.pkl')
        pickle.dump(artwork_content_model, open(artwork_filename, 'wb'))
        print(f'\nArtwork Content model saved :{artwork_filename}')

        
    except Exception as e:
        # Write out an error file. This will be returned as the failureReason in the
        # DescribeTrainingJob result.
        trc = traceback.format_exc()
        with open(os.path.join(output_path, 'failure'), 'w') as s:
            s.write('Exception during training: ' + str(e) + '\n' + trc)
        # Printing this causes the exception to be in the training job logs, as well.
        print('Exception during training: ' + str(e) + '\n' + trc, file=sys.stderr)
        # A non-zero exit code causes the training job to be marked as Failed.
        sys.exit(255)

if __name__ == '__main__':
    train()

    # A zero exit code causes the job to be marked a Succeeded.
    sys.exit(0)
